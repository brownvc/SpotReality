{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU, Softmax\n",
    "import numpy as np\n",
    "import json\n",
    "import socket\n",
    "from time import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple Neural Net\n",
    "Initially, input is features, output is action\n",
    "\"\"\"\n",
    "FEATURE_NUM = 14\n",
    "ACTION_NUM = 10\n",
    "\n",
    "class ActorCritic:\n",
    "    def __init__(self):                 \n",
    "\n",
    "        self.device = torch.device(\"cuda\")\n",
    "\n",
    "        actor_size = 128\n",
    "        self.actor = Sequential(\n",
    "            # feature size by hidden size\n",
    "            Linear(FEATURE_NUM, actor_size),\n",
    "            ReLU(),\n",
    "            # Linear(actor_size, actor_size),\n",
    "            # ReLU(),\n",
    "            # hidden size by action size\n",
    "            Linear(actor_size, ACTION_NUM),\n",
    "            Softmax(dim=-1)\n",
    "        ).to(self.device)\n",
    "\n",
    "        critic_size = 32\n",
    "        self.critic = Sequential(\n",
    "            # feature size by hidden size\n",
    "            Linear(FEATURE_NUM, critic_size),\n",
    "            ReLU(),\n",
    "            Linear(critic_size, critic_size),\n",
    "            ReLU(),\n",
    "            # hidden size by action size\n",
    "            Linear(critic_size, 1),\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-4)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-4)\n",
    "        self.gamma = 0.996\n",
    "        self.num_backward = 0\n",
    "        self.total_steps = 0\n",
    "        self.total_reward = 0\n",
    "        self.ep_steps = 0\n",
    "        self.total_loss = [0,0]\n",
    "\n",
    "        # If weights already exist, load them\n",
    "        if os.path.exists(\"pretrained_weights.pt\"):\n",
    "            self.neural_net.load_state_dict(torch.load(\"pretrained_weights.pt\", map_location=\"cuda:0\"))\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        print(\"COMPLETED EPISODE IN \" + str(self.ep_steps) + \" STEPS!\")\n",
    "        print(\"TOTAL REWARD: \" + str(self.total_reward))\n",
    "        print(\"TOTAL LOSS (ACTOR, CRITIC): \" + str(self.total_loss))\n",
    "        self.ep_steps = 0\n",
    "        self.total_reward = 0\n",
    "        self.total_loss = [0,0]\n",
    "        self.state = None\n",
    "        self.reward = None\n",
    "        self.action = None\n",
    "\n",
    "    def forward(self, input_feats):\n",
    "        # Take as input a featurized state, output action probs\n",
    "        probs = self.actor(input_feats.to(self.device))\n",
    "        # Randomly select action according to probs\n",
    "        return probs\n",
    "    \n",
    "    def value(self, input_feats):\n",
    "        value = self.critic(input_feats.to(self.device))\n",
    "        return value\n",
    "    \n",
    "    def get_action(self, input_feats):\n",
    "        # samples an action from the softmax distribution\n",
    "        self.total_steps += 1\n",
    "        self.ep_steps += 1\n",
    "        probs = self.forward(input_feats)\n",
    "        action = np.random.choice(probs.shape[0], p=probs.cpu().detach().numpy())\n",
    "        # action = int(np.argmax(probs.cpu().detach().numpy())) # Deterministic\n",
    "        self.state = input_feats\n",
    "        self.action = action\n",
    "        return action\n",
    "    \n",
    "    def loss(self, next_state):\n",
    "\n",
    "        action_probs = self.forward(self.state)\n",
    "\n",
    "        cur_value = self.value(self.state)\n",
    "        next_value = self.value(next_state)\n",
    "\n",
    "        advantage = self.reward + self.gamma*next_value - cur_value\n",
    "        advantage_copy = advantage.clone()      \n",
    "        actor_loss = -advantage * torch.log(action_probs[self.action])\n",
    "        critic_loss = torch.square(advantage_copy)\n",
    "\n",
    "        self.total_loss[0] = self.total_loss[0] + actor_loss.tolist()[0] \n",
    "        self.total_loss[1] = self.total_loss[1] + critic_loss.tolist()[0]\n",
    "\n",
    "        return (actor_loss, critic_loss)\n",
    "\n",
    "    def backward(self, next_state):\n",
    "        # Ignore if websocket returned for no reason\n",
    "        # Might need both current and previous state\n",
    "        # Terminal, don't want to do a backward pass on next\n",
    "        self.num_backward += 1\n",
    "\n",
    "        actor_loss, critic_loss = self.loss(next_state)\n",
    "\n",
    "        if self.reward != None:\n",
    "            if self.total_reward != None:\n",
    "                self.total_reward += self.reward\n",
    "            else:\n",
    "                self.total_reward = self.reward\n",
    "        \n",
    "        # loss = actor_loss + critic_loss # might cause errors\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Every 10 backward passes, save the model\n",
    "        if self.num_backward % 5 == 0:\n",
    "            # print(f\"Saving model. Num total steps={self.total_steps}. Num backward passes={self.num_backward}\")\n",
    "            # print(f\"Total rewards: \" + str(self.total_reward))\n",
    "            # print(f\"Current Reward: \" + str(self.reward))\n",
    "            start = time()\n",
    "            torch.save(self.actor.state_dict(), \"actor_pretrained_weights.pt\")\n",
    "            torch.save(self.critic.state_dict(), \"critic_pretrained_weights.pt\")\n",
    "        return 0\n",
    "\n",
    "    def discounted_return(self, rewards):\n",
    "        return np.dot(self.gamma ** np.arange(len(rewards)), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle json\n",
    "\n",
    "def convert_to_feats(response_dict):\n",
    "    \"\"\"Takes a response dictionary and returns tensor of features\"\"\"\n",
    "    return torch.Tensor(\n",
    "        (\n",
    "            response_dict['feat1'],\n",
    "            response_dict['feat2'],\n",
    "            response_dict['feat3'],\n",
    "            response_dict['feat4'],\n",
    "            response_dict['feat5'],\n",
    "            response_dict['feat6'],\n",
    "            response_dict['feat7'],\n",
    "            response_dict['feat8'],\n",
    "            response_dict['feat9'],\n",
    "            response_dict['feat10'],\n",
    "            response_dict['feat11'],\n",
    "            response_dict['feat12'],\n",
    "            response_dict['feat13'],\n",
    "            response_dict['feat14'],\n",
    "        )\n",
    "    )\n",
    "\n",
    "def handle_response(response, net: ActorCritic):\n",
    "    \"\"\"Take as input a websocket response from C#, return serialized data to send back\"\"\"\n",
    "    # Initialize response\n",
    "    instruction = ''\n",
    "    action = 0\n",
    "    if response is None:\n",
    "        print(\"Response none\")\n",
    "        return {}\n",
    "    response_dict = json.loads(response)\n",
    "\n",
    "    # Store the reward for the previous action. \n",
    "    # Don't worry about the first one in the array, it will be ignored\n",
    "    net.reward = response_dict['reward']\n",
    "\n",
    "    # Handle forward pass\n",
    "    if response_dict['instruction'] == 'forward':\n",
    "        features = convert_to_feats(response_dict)\n",
    "        if net.state != None:\n",
    "            net.backward(features)\n",
    "        action = net.get_action(features)\n",
    "        instruction = 'step'\n",
    "    elif response_dict['instruction'] == 'update_weights':\n",
    "        instruction = 'reset'\n",
    "\n",
    "        features = convert_to_feats(response_dict)\n",
    "        net.backward(features)\n",
    "        net.reset()\n",
    "        # print(f\"Took {time() - start:.4f} seconds to compute backward pass\")        \n",
    "\n",
    "    json_return_dict = {\n",
    "        'instruction': instruction,\n",
    "        'action': action,\n",
    "    }\n",
    "    \n",
    "    return json_return_dict\n",
    "\n",
    "## TODO\n",
    "# Video of training randomly and video of training close to final value\n",
    "# Throw in graph of number of steps (maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED EPISODE IN 0 STEPS!\n",
      "TOTAL REWARD: 0\n",
      "TOTAL LOSS (ACTOR, CRITIC): [0, 0]\n",
      "COMPLETED EPISODE IN 330 STEPS!\n",
      "TOTAL REWARD: 482.75\n",
      "TOTAL LOSS (ACTOR, CRITIC): [1104.842864215374, 562777.7276251083]\n",
      "COMPLETED EPISODE IN 2001 STEPS!\n",
      "TOTAL REWARD: -1998.0\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-4581.960186004639, 1996.9066520482302]\n",
      "COMPLETED EPISODE IN 331 STEPS!\n",
      "TOTAL REWARD: 536.375\n",
      "TOTAL LOSS (ACTOR, CRITIC): [1184.526533216238, 562661.8240091214]\n",
      "COMPLETED EPISODE IN 2001 STEPS!\n",
      "TOTAL REWARD: -1927.0\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-4399.661815881729, 1890.1386563926935]\n",
      "COMPLETED EPISODE IN 2001 STEPS!\n",
      "TOTAL REWARD: -1552.0\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-3554.784764945507, 1369.2233520001173]\n",
      "COMPLETED EPISODE IN 2001 STEPS!\n",
      "TOTAL REWARD: -1821.0\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-4167.042239129543, 1730.7334525510669]\n",
      "COMPLETED EPISODE IN 2001 STEPS!\n",
      "TOTAL REWARD: -1380.5\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-3149.8460753262043, 1077.5979666411877]\n",
      "COMPLETED EPISODE IN 2001 STEPS!\n",
      "TOTAL REWARD: -1615.25\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-3646.075417339802, 1447.0319062769413]\n",
      "COMPLETED EPISODE IN 432 STEPS!\n",
      "TOTAL REWARD: 525.875\n",
      "TOTAL LOSS (ACTOR, CRITIC): [1143.723960518837, 562615.202429343]\n",
      "COMPLETED EPISODE IN 556 STEPS!\n",
      "TOTAL REWARD: 468.5\n",
      "TOTAL LOSS (ACTOR, CRITIC): [928.0471390485764, 562598.9698963799]\n",
      "COMPLETED EPISODE IN 315 STEPS!\n",
      "TOTAL REWARD: 578.25\n",
      "TOTAL LOSS (ACTOR, CRITIC): [1095.5524938255548, 562480.8738415996]\n",
      "COMPLETED EPISODE IN 2001 STEPS!\n",
      "TOTAL REWARD: -1078.25\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-2446.674885213375, 732.7242115177214]\n",
      "COMPLETED EPISODE IN 342 STEPS!\n",
      "TOTAL REWARD: 545.5\n",
      "TOTAL LOSS (ACTOR, CRITIC): [962.6237433701754, 562501.6529537356]\n",
      "COMPLETED EPISODE IN 733 STEPS!\n",
      "TOTAL REWARD: 427.5\n",
      "TOTAL LOSS (ACTOR, CRITIC): [586.9346881955862, 562470.2327586589]\n",
      "COMPLETED EPISODE IN 871 STEPS!\n",
      "TOTAL REWARD: 255.875\n",
      "TOTAL LOSS (ACTOR, CRITIC): [128.02957339584827, 562569.4766532481]\n",
      "COMPLETED EPISODE IN 236 STEPS!\n",
      "TOTAL REWARD: 602.375\n",
      "TOTAL LOSS (ACTOR, CRITIC): [842.4874054789543, 562329.8465862083]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;66;03m#print(f\"Took {time() - start:.4f} between, that's {1/(time() - start):.2f} Hz\")\u001b[39;00m\n\u001b[0;32m     27\u001b[0m         start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m---> 28\u001b[0m         request \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneural_net\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     sock\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[48], line 42\u001b[0m, in \u001b[0;36mhandle_response\u001b[1;34m(response, net)\u001b[0m\n\u001b[0;32m     40\u001b[0m features \u001b[38;5;241m=\u001b[39m convert_to_feats(response_dict)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m net\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m action \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mget_action(features)\n\u001b[0;32m     44\u001b[0m instruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[47], line 118\u001b[0m, in \u001b[0;36mActorCritic.backward\u001b[1;34m(self, next_state)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 118\u001b[0m \u001b[43mactor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\jtoribio\\miniconda3\\envs\\SpotRl\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jtoribio\\miniconda3\\envs\\SpotRl\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jtoribio\\miniconda3\\envs\\SpotRl\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "host, port = \"127.0.0.1\", 25001\n",
    "\n",
    "# SOCK_STREAM means TCP socket\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "neural_net = ActorCritic()\n",
    "request = {\n",
    "    'instruction': 'init',\n",
    "    'action': 0,\n",
    "}\n",
    "\n",
    "def on_receive(message):\n",
    "    print(message)\n",
    "\n",
    "try:\n",
    "    # Connect to the server and send the data\n",
    "    sock.connect((host, port))\n",
    "\n",
    "    start = 0\n",
    "\n",
    "    while(True):\n",
    "        #print(request)\n",
    "        request = json.dumps(request)\n",
    "        sock.sendall(request.encode(\"utf-8\"))\n",
    "        response = sock.recv(1024).decode(\"utf-8\")\n",
    "        #print(f\"Took {time() - start:.4f} between, that's {1/(time() - start):.2f} Hz\")\n",
    "        start = time()\n",
    "        request = handle_response(response, neural_net)\n",
    "finally:\n",
    "    sock.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
