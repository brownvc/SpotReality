{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU, Softmax\n",
    "import numpy as np\n",
    "import json\n",
    "import socket\n",
    "from time import time\n",
    "import os\n",
    "os.system(\"pip install matplotlib\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple Neural Net\n",
    "Initially, input is features, output is action\n",
    "\"\"\"\n",
    "FEATURE_NUM = 14\n",
    "ACTION_NUM = 10\n",
    "\n",
    "\n",
    "class ActorCritic:\n",
    "    def __init__(self):                 \n",
    "\n",
    "        self.device = torch.device(\"cuda\")\n",
    "\n",
    "        actor_size = 128\n",
    "        self.actor = Sequential(\n",
    "            # feature size by hidden size\n",
    "            Linear(FEATURE_NUM, actor_size),\n",
    "            ReLU(),\n",
    "            # Linear(actor_size, actor_size),\n",
    "            # ReLU(),\n",
    "            # hidden size by action size\n",
    "            Linear(actor_size, ACTION_NUM),\n",
    "            Softmax(dim=-1)\n",
    "        ).to(self.device)\n",
    "\n",
    "        critic_size = 32\n",
    "        self.critic = Sequential(\n",
    "            # feature size by hidden size\n",
    "            Linear(FEATURE_NUM, critic_size),\n",
    "            ReLU(),\n",
    "            Linear(critic_size, critic_size),\n",
    "            ReLU(),\n",
    "            # hidden size by action size\n",
    "            Linear(critic_size, 1),\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=5e-3)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "        self.gamma = 0.996\n",
    "        self.num_backward = 0\n",
    "        self.total_steps = 0\n",
    "        self.total_reward = 0\n",
    "        self.ep_steps = 0\n",
    "        self.total_loss = [0,0]\n",
    "\n",
    "        self.ep_count = 0\n",
    "        self.plt_x = []\n",
    "        self.plt_y = []\n",
    "\n",
    "        # If weights already exist, load them\n",
    "        if os.path.exists(\"critic_pretrained_weights.pt\"):\n",
    "            self.critic.load_state_dict(torch.load(\"critic_pretrained_weights.pt\", map_location=\"cuda:0\"))\n",
    "        if os.path.exists(\"actor_pretrained_weights.pt\"):\n",
    "            self.actor.load_state_dict(torch.load(\"actor_pretrained_weights.pt\", map_location=\"cuda:0\"))\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.ep_count += 1\n",
    "        print(\"COMPLETED EPISODE IN \" + str(self.ep_steps) + \" STEPS!\")\n",
    "        print(\"TOTAL REWARD: \" + str(self.total_reward))\n",
    "        print(\"TOTAL LOSS (ACTOR, CRITIC): \" + str(self.total_loss))\n",
    "        self.plt_x = np.append(self.plt_x, self.ep_count)\n",
    "        self.plt_y = np.append(self.plt_y, self.ep_steps)\n",
    "\n",
    "        self.ep_steps = 0\n",
    "        self.total_reward = 0\n",
    "        self.total_loss = [0,0]\n",
    "        self.state = None\n",
    "        self.reward = None\n",
    "        self.action = None\n",
    "\n",
    "\n",
    "    def forward(self, input_feats):\n",
    "        # Take as input a featurized state, output action probs\n",
    "        probs = self.actor(input_feats.to(self.device))\n",
    "        # Randomly select action according to probs\n",
    "        return probs\n",
    "    \n",
    "    def value(self, input_feats):\n",
    "        value = self.critic(input_feats.to(self.device))\n",
    "        return value\n",
    "    \n",
    "    def get_action(self, input_feats):\n",
    "        # samples an action from the softmax distribution\n",
    "        self.total_steps += 1\n",
    "        self.ep_steps += 1\n",
    "        probs = self.forward(input_feats)\n",
    "        action = np.random.choice(probs.shape[0], p=probs.cpu().detach().numpy())\n",
    "        # action = int(np.argmax(probs.cpu().detach().numpy())) # Deterministic\n",
    "        self.state = input_feats\n",
    "        self.action = action\n",
    "        return action\n",
    "    \n",
    "    def loss(self, next_state):\n",
    "\n",
    "        action_probs = self.forward(self.state)\n",
    "\n",
    "        cur_value = self.value(self.state)\n",
    "        next_value = self.value(next_state)\n",
    "\n",
    "        advantage = self.reward + self.gamma*next_value - cur_value\n",
    "        # print(\"Reward: \" + str(self.reward) + \"  ///  next_value: \" + str(next_value) + \"  ///  cur_value: \" + str(cur_value))\n",
    "        \n",
    "        advantage_copy = advantage.clone()\n",
    "        # actor_loss = torch.abs(advantage * torch.log(action_probs[self.action]))\n",
    "        actor_loss = -advantage * torch.log(action_probs[self.action]) # ORIGINAL\n",
    "        # print(\"Log Probs: \" + str(torch.log(action_probs[self.action])))\n",
    "        # critic_loss = -advantage_copy\n",
    "        critic_loss = torch.square(advantage_copy) # ORIGINAL\n",
    "\n",
    "        self.total_loss[0] = self.total_loss[0] + actor_loss.tolist()[0] \n",
    "        self.total_loss[1] = self.total_loss[1] + critic_loss.tolist()[0]\n",
    "\n",
    "        return (actor_loss, critic_loss)\n",
    "\n",
    "    def backward(self, next_state):\n",
    "        # Ignore if websocket returned for no reason\n",
    "        # Might need both current and previous state\n",
    "        # Terminal, don't want to do a backward pass on next\n",
    "        self.num_backward += 1\n",
    "\n",
    "        actor_loss, critic_loss = self.loss(next_state)\n",
    "\n",
    "        if self.reward != None:\n",
    "            if self.total_reward != None:\n",
    "                self.total_reward += self.reward\n",
    "            else:\n",
    "                self.total_reward = self.reward\n",
    "        \n",
    "        # loss = actor_loss + critic_loss # might cause errors\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Every 10 backward passes, save the model\n",
    "        if self.num_backward % 5 == 0:\n",
    "            # print(f\"Saving model. Num total steps={self.total_steps}. Num backward passes={self.num_backward}\")\n",
    "            # print(f\"Total rewards: \" + str(self.total_reward))\n",
    "            # print(f\"Current Reward: \" + str(self.reward))\n",
    "            start = time()\n",
    "            torch.save(self.actor.state_dict(), \"actor_pretrained_weights.pt\")\n",
    "            torch.save(self.critic.state_dict(), \"critic_pretrained_weights.pt\")\n",
    "        return 0\n",
    "\n",
    "    def discounted_return(self, rewards):\n",
    "        return np.dot(self.gamma ** np.arange(len(rewards)), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle json\n",
    "\n",
    "def convert_to_feats(response_dict):\n",
    "    \"\"\"Takes a response dictionary and returns tensor of features\"\"\"\n",
    "    return torch.Tensor(\n",
    "        (\n",
    "            response_dict['feat1'],\n",
    "            response_dict['feat2'],\n",
    "            response_dict['feat3'],\n",
    "            response_dict['feat4'],\n",
    "            response_dict['feat5'],\n",
    "            response_dict['feat6'],\n",
    "            response_dict['feat7'],\n",
    "            response_dict['feat8'],\n",
    "            response_dict['feat9'],\n",
    "            response_dict['feat10'],\n",
    "            response_dict['feat11'],\n",
    "            response_dict['feat12'],\n",
    "            response_dict['feat13'],\n",
    "            response_dict['feat14'],\n",
    "        )\n",
    "    )\n",
    "\n",
    "def handle_response(response, net: ActorCritic):\n",
    "    \"\"\"Take as input a websocket response from C#, return serialized data to send back\"\"\"\n",
    "    # Initialize response\n",
    "    instruction = ''\n",
    "    action = 0\n",
    "    if response is None:\n",
    "        print(\"Response none\")\n",
    "        return {}\n",
    "    response_dict = json.loads(response)\n",
    "\n",
    "    # Store the reward for the previous action. \n",
    "    # Don't worry about the first one in the array, it will be ignored\n",
    "    net.reward = response_dict['reward']\n",
    "\n",
    "    # Handle forward pass\n",
    "    if response_dict['instruction'] == 'forward':\n",
    "        features = convert_to_feats(response_dict)\n",
    "        if net.state != None:\n",
    "            net.backward(features)\n",
    "        action = net.get_action(features)\n",
    "        instruction = 'step'\n",
    "    elif response_dict['instruction'] == 'update_weights':\n",
    "        instruction = 'reset'\n",
    "\n",
    "        features = convert_to_feats(response_dict)\n",
    "        net.backward(features)\n",
    "        net.reset()\n",
    "        # print(f\"Took {time() - start:.4f} seconds to compute backward pass\")        \n",
    "\n",
    "    json_return_dict = {\n",
    "        'instruction': instruction,\n",
    "        'action': action,\n",
    "    }\n",
    "    \n",
    "    return json_return_dict\n",
    "\n",
    "## TODO\n",
    "# Video of training randomly and video of training close to final value\n",
    "# Throw in graph of number of steps (maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED EPISODE IN 0 STEPS!\n",
      "TOTAL REWARD: 0\n",
      "TOTAL LOSS (ACTOR, CRITIC): [0, 0]\n",
      "COMPLETED EPISODE IN 778 STEPS!\n",
      "TOTAL REWARD: -483.5645834505558\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-1099.7110334336758, 327.4642194136977]\n",
      "COMPLETED EPISODE IN 359 STEPS!\n",
      "TOTAL REWARD: -218.01548682153225\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-493.4163213670254, 160.5975377857685]\n",
      "COMPLETED EPISODE IN 1212 STEPS!\n",
      "TOTAL REWARD: -615.4492455124855\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-1382.303859859705, 330.50096971169114]\n",
      "COMPLETED EPISODE IN 254 STEPS!\n",
      "TOTAL REWARD: -126.61238904297352\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-287.41009083390236, 64.99629357084632]\n",
      "COMPLETED EPISODE IN 348 STEPS!\n",
      "TOTAL REWARD: -188.84294673800468\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-427.92353242635727, 107.80612526834011]\n",
      "COMPLETED EPISODE IN 706 STEPS!\n",
      "TOTAL REWARD: -445.64764080941677\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-988.2347196638584, 310.2501282468438]\n",
      "COMPLETED EPISODE IN 236 STEPS!\n",
      "TOTAL REWARD: -109.31426711380482\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-244.5301434993744, 51.98245947062969]\n",
      "COMPLETED EPISODE IN 79 STEPS!\n",
      "TOTAL REWARD: -41.32789345085621\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-92.08474111557007, 22.676801320165396]\n",
      "COMPLETED EPISODE IN 88 STEPS!\n",
      "TOTAL REWARD: -34.92215660214424\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-76.83542862534523, 15.369577996432781]\n",
      "COMPLETED EPISODE IN 113 STEPS!\n",
      "TOTAL REWARD: -57.02916598320007\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-119.91583952307701, 30.102747455239296]\n",
      "COMPLETED EPISODE IN 461 STEPS!\n",
      "TOTAL REWARD: -237.71658924221992\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-529.2295034527779, 141.14305556192994]\n",
      "COMPLETED EPISODE IN 647 STEPS!\n",
      "TOTAL REWARD: -339.31183049082756\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-727.4846195876598, 202.51618180796504]\n",
      "COMPLETED EPISODE IN 672 STEPS!\n",
      "TOTAL REWARD: -344.15171456336975\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-764.5429862141609, 187.47273400053382]\n",
      "COMPLETED EPISODE IN 1323 STEPS!\n",
      "TOTAL REWARD: -670.9078200906515\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-1484.7641848623753, 365.2005366869271]\n",
      "COMPLETED EPISODE IN 200 STEPS!\n",
      "TOTAL REWARD: -132.69819082319736\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-277.3789146244526, 91.37312577292323]\n",
      "COMPLETED EPISODE IN 606 STEPS!\n",
      "TOTAL REWARD: -398.5268541276455\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-869.3412487208843, 268.7766886278987]\n",
      "COMPLETED EPISODE IN 150 STEPS!\n",
      "TOTAL REWARD: -105.69781813025475\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-226.4836477637291, 79.29978158324957]\n",
      "COMPLETED EPISODE IN 205 STEPS!\n",
      "TOTAL REWARD: -138.0899152904749\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-298.1618338525295, 101.62985328957438]\n",
      "COMPLETED EPISODE IN 369 STEPS!\n",
      "TOTAL REWARD: -206.4755324870348\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-448.4660542309284, 116.58512987941504]\n",
      "COMPLETED EPISODE IN 818 STEPS!\n",
      "TOTAL REWARD: -400.81838765740395\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-875.711896032095, 211.80979716032743]\n",
      "COMPLETED EPISODE IN 621 STEPS!\n",
      "TOTAL REWARD: -302.33064752817154\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-650.4786974787712, 144.23603405617177]\n",
      "COMPLETED EPISODE IN 843 STEPS!\n",
      "TOTAL REWARD: -470.19719809293747\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-1004.8897762596607, 274.0128638166934]\n",
      "COMPLETED EPISODE IN 267 STEPS!\n",
      "TOTAL REWARD: -126.54353034496307\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-269.62831500172615, 56.38220800459385]\n",
      "COMPLETED EPISODE IN 136 STEPS!\n",
      "TOTAL REWARD: -56.96854603290558\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-120.63543829321861, 24.084329906851053]\n",
      "COMPLETED EPISODE IN 780 STEPS!\n",
      "TOTAL REWARD: -352.70080426335335\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-744.843830704689, 154.2387269437313]\n",
      "COMPLETED EPISODE IN 65 STEPS!\n",
      "TOTAL REWARD: -31.161435648798943\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-65.46039091050625, 14.610541437752545]\n",
      "COMPLETED EPISODE IN 182 STEPS!\n",
      "TOTAL REWARD: -85.07645864784718\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-180.49700197577477, 36.382787043228745]\n",
      "COMPLETED EPISODE IN 207 STEPS!\n",
      "TOTAL REWARD: -84.55135923624039\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-176.78839096426964, 30.783700346946716]\n",
      "COMPLETED EPISODE IN 37 STEPS!\n",
      "TOTAL REWARD: -18.76357200741768\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-38.57751390337944, 8.728980090469122]\n",
      "COMPLETED EPISODE IN 593 STEPS!\n",
      "TOTAL REWARD: -342.1940035223961\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-719.4033246040344, 179.4674547482282]\n",
      "COMPLETED EPISODE IN 766 STEPS!\n",
      "TOTAL REWARD: -448.45756378769875\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-930.2887894511223, 262.80913230217993]\n",
      "COMPLETED EPISODE IN 536 STEPS!\n",
      "TOTAL REWARD: -268.6057797521353\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-539.3452150076628, 117.98136124573648]\n",
      "COMPLETED EPISODE IN 111 STEPS!\n",
      "TOTAL REWARD: -75.85943637788296\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-157.74981719255447, 48.782594388350844]\n",
      "COMPLETED EPISODE IN 209 STEPS!\n",
      "TOTAL REWARD: -126.1071204841137\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-247.80714938044548, 76.16246750950813]\n",
      "COMPLETED EPISODE IN 429 STEPS!\n",
      "TOTAL REWARD: -225.21133895218372\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-428.4801250696182, 101.59457977139391]\n",
      "COMPLETED EPISODE IN 245 STEPS!\n",
      "TOTAL REWARD: -108.41203068196774\n",
      "TOTAL LOSS (ACTOR, CRITIC): [-200.74172581732273, 41.69014713168144]\n"
     ]
    }
   ],
   "source": [
    "host, port = \"127.0.0.1\", 25001\n",
    "\n",
    "# SOCK_STREAM means TCP socket\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "neural_net = ActorCritic()\n",
    "request = {\n",
    "    'instruction': 'init',\n",
    "    'action': 0,\n",
    "}\n",
    "\n",
    "def on_receive(message):\n",
    "    print(message)\n",
    "\n",
    "try:\n",
    "    # Connect to the server and send the data\n",
    "    sock.connect((host, port))\n",
    "\n",
    "    start = 0\n",
    "\n",
    "    from time import time\n",
    "\n",
    "    while(True):\n",
    "        # print(request)\n",
    "        request = json.dumps(request)\n",
    "        sock.sendall(request.encode(\"utf-8\"))\n",
    "        response = sock.recv(1024).decode(\"utf-8\")\n",
    "        # print(f\"Took {time() - start:.4f} between, that's {1/(time() - start):.2f} Hz\")\n",
    "        start = time()\n",
    "        request = handle_response(response, neural_net)\n",
    "\n",
    "        if neural_net.ep_count == 40:\n",
    "            plt.plot(neural_net.plt_x, neural_net.plt_y)\n",
    "            plt.xlabel(\"Episode Number\")\n",
    "            plt.ylabel(\"Steps per Episode\")\n",
    "            plt.show()\n",
    "            plt.savefig(\"actor_critic_plot.png\")\n",
    "finally:\n",
    "    sock.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
