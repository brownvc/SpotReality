{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU, Softmax\n",
    "import numpy as np\n",
    "import json\n",
    "import socket\n",
    "from time import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple Neural Net\n",
    "Initially, input is features, output is action\n",
    "\"\"\"\n",
    "FEATURE_NUM = 14\n",
    "ACTION_NUM = 10\n",
    "\n",
    "class ActorCritic:\n",
    "    def __init__(self):                 \n",
    "\n",
    "        self.device = torch.device(\"cuda\")\n",
    "\n",
    "        actor_size = 128\n",
    "        self.actor = Sequential(\n",
    "            # feature size by hidden size\n",
    "            Linear(FEATURE_NUM, actor_size),\n",
    "            ReLU(),\n",
    "            # Linear(actor_size, actor_size),\n",
    "            # ReLU(),\n",
    "            # hidden size by action size\n",
    "            Linear(actor_size, ACTION_NUM),\n",
    "            Softmax(dim=-1)\n",
    "        ).to(self.device)\n",
    "\n",
    "        critic_size = 32\n",
    "        self.critic = Sequential(\n",
    "            # feature size by hidden size\n",
    "            Linear(FEATURE_NUM, critic_size),\n",
    "            ReLU(),\n",
    "            Linear(critic_size, critic_size),\n",
    "            ReLU(),\n",
    "            # hidden size by action size\n",
    "            Linear(critic_size, 1),\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=5e-4)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-4)\n",
    "        self.gamma = 0.996\n",
    "        self.num_backward = 0\n",
    "        self.total_steps = 0\n",
    "        self.total_reward = 0\n",
    "        self.ep_steps = 0\n",
    "        self.total_loss = [0,0]\n",
    "\n",
    "        # If weights already exist, load them\n",
    "        if os.path.exists(\"critic_pretrained_weights.pt\"):\n",
    "            self.critic.load_state_dict(torch.load(\"critic_pretrained_weights.pt\", map_location=\"cuda:0\"))\n",
    "        if os.path.exists(\"actor_pretrained_weights.pt\"):\n",
    "            self.actor.load_state_dict(torch.load(\"actor_pretrained_weights.pt\", map_location=\"cuda:0\"))\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        print(\"COMPLETED EPISODE IN \" + str(self.ep_steps) + \" STEPS!\")\n",
    "        print(\"TOTAL REWARD: \" + str(self.total_reward))\n",
    "        print(\"TOTAL LOSS (ACTOR, CRITIC): \" + str(self.total_loss))\n",
    "        self.ep_steps = 0\n",
    "        self.total_reward = 0\n",
    "        self.total_loss = [0,0]\n",
    "        self.state = None\n",
    "        self.reward = None\n",
    "        self.action = None\n",
    "\n",
    "    def forward(self, input_feats):\n",
    "        # Take as input a featurized state, output action probs\n",
    "        probs = self.actor(input_feats.to(self.device))\n",
    "        # Randomly select action according to probs\n",
    "        return probs\n",
    "    \n",
    "    def value(self, input_feats):\n",
    "        value = self.critic(input_feats.to(self.device))\n",
    "        return value\n",
    "    \n",
    "    def get_action(self, input_feats):\n",
    "        # samples an action from the softmax distribution\n",
    "        self.total_steps += 1\n",
    "        self.ep_steps += 1\n",
    "        probs = self.forward(input_feats)\n",
    "        action = np.random.choice(probs.shape[0], p=probs.cpu().detach().numpy())\n",
    "        # action = int(np.argmax(probs.cpu().detach().numpy())) # Deterministic\n",
    "        self.state = input_feats\n",
    "        self.action = action\n",
    "        return action\n",
    "    \n",
    "    def loss(self, next_state):\n",
    "\n",
    "        action_probs = self.forward(self.state)\n",
    "\n",
    "        cur_value = self.value(self.state)\n",
    "        next_value = self.value(next_state)\n",
    "\n",
    "        advantage = self.reward + self.gamma*next_value - cur_value\n",
    "        # print(\"Reward: \" + str(self.reward) + \"  ///  next_value: \" + str(next_value) + \"  ///  cur_value: \" + str(cur_value))\n",
    "        \n",
    "        advantage_copy = advantage.clone()\n",
    "        # actor_loss = torch.abs(advantage * torch.log(action_probs[self.action]))\n",
    "        actor_loss = -advantage * torch.log(action_probs[self.action]) # ORIGINAL\n",
    "        # print(\"Log Probs: \" + str(torch.log(action_probs[self.action])))\n",
    "        # critic_loss = -advantage_copy\n",
    "        critic_loss = torch.square(advantage_copy) # ORIGINAL\n",
    "\n",
    "        self.total_loss[0] = self.total_loss[0] + actor_loss.tolist()[0] \n",
    "        self.total_loss[1] = self.total_loss[1] + critic_loss.tolist()[0]\n",
    "\n",
    "        return (actor_loss, critic_loss)\n",
    "\n",
    "    def backward(self, next_state):\n",
    "        # Ignore if websocket returned for no reason\n",
    "        # Might need both current and previous state\n",
    "        # Terminal, don't want to do a backward pass on next\n",
    "        self.num_backward += 1\n",
    "\n",
    "        actor_loss, critic_loss = self.loss(next_state)\n",
    "\n",
    "        if self.reward != None:\n",
    "            if self.total_reward != None:\n",
    "                self.total_reward += self.reward\n",
    "            else:\n",
    "                self.total_reward = self.reward\n",
    "        \n",
    "        # loss = actor_loss + critic_loss # might cause errors\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Every 10 backward passes, save the model\n",
    "        if self.num_backward % 5 == 0:\n",
    "            # print(f\"Saving model. Num total steps={self.total_steps}. Num backward passes={self.num_backward}\")\n",
    "            # print(f\"Total rewards: \" + str(self.total_reward))\n",
    "            # print(f\"Current Reward: \" + str(self.reward))\n",
    "            start = time()\n",
    "            torch.save(self.actor.state_dict(), \"actor_pretrained_weights.pt\")\n",
    "            torch.save(self.critic.state_dict(), \"critic_pretrained_weights.pt\")\n",
    "        return 0\n",
    "\n",
    "    def discounted_return(self, rewards):\n",
    "        return np.dot(self.gamma ** np.arange(len(rewards)), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle json\n",
    "\n",
    "def convert_to_feats(response_dict):\n",
    "    \"\"\"Takes a response dictionary and returns tensor of features\"\"\"\n",
    "    return torch.Tensor(\n",
    "        (\n",
    "            response_dict['feat1'],\n",
    "            response_dict['feat2'],\n",
    "            response_dict['feat3'],\n",
    "            response_dict['feat4'],\n",
    "            response_dict['feat5'],\n",
    "            response_dict['feat6'],\n",
    "            response_dict['feat7'],\n",
    "            response_dict['feat8'],\n",
    "            response_dict['feat9'],\n",
    "            response_dict['feat10'],\n",
    "            response_dict['feat11'],\n",
    "            response_dict['feat12'],\n",
    "            response_dict['feat13'],\n",
    "            response_dict['feat14'],\n",
    "        )\n",
    "    )\n",
    "\n",
    "def handle_response(response, net: ActorCritic):\n",
    "    \"\"\"Take as input a websocket response from C#, return serialized data to send back\"\"\"\n",
    "    # Initialize response\n",
    "    instruction = ''\n",
    "    action = 0\n",
    "    if response is None:\n",
    "        print(\"Response none\")\n",
    "        return {}\n",
    "    response_dict = json.loads(response)\n",
    "\n",
    "    # Store the reward for the previous action. \n",
    "    # Don't worry about the first one in the array, it will be ignored\n",
    "    net.reward = response_dict['reward']\n",
    "\n",
    "    # Handle forward pass\n",
    "    if response_dict['instruction'] == 'forward':\n",
    "        features = convert_to_feats(response_dict)\n",
    "        if net.state != None:\n",
    "            net.backward(features)\n",
    "        action = net.get_action(features)\n",
    "        instruction = 'step'\n",
    "    elif response_dict['instruction'] == 'update_weights':\n",
    "        instruction = 'reset'\n",
    "\n",
    "        features = convert_to_feats(response_dict)\n",
    "        net.backward(features)\n",
    "        net.reset()\n",
    "        # print(f\"Took {time() - start:.4f} seconds to compute backward pass\")        \n",
    "\n",
    "    json_return_dict = {\n",
    "        'instruction': instruction,\n",
    "        'action': action,\n",
    "    }\n",
    "    \n",
    "    return json_return_dict\n",
    "\n",
    "## TODO\n",
    "# Video of training randomly and video of training close to final value\n",
    "# Throw in graph of number of steps (maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED EPISODE IN 0 STEPS!\n",
      "TOTAL REWARD: 0\n",
      "TOTAL LOSS (ACTOR, CRITIC): [0, 0]\n",
      "COMPLETED EPISODE IN 709 STEPS!\n",
      "TOTAL REWARD: 397.58700449168884\n",
      "TOTAL LOSS (ACTOR, CRITIC): [911.6533510088921, 397.5269535779953]\n",
      "COMPLETED EPISODE IN 314 STEPS!\n",
      "TOTAL REWARD: 609.6574376322329\n",
      "TOTAL LOSS (ACTOR, CRITIC): [1356.366232059896, 609.4730781242251]\n",
      "COMPLETED EPISODE IN 256 STEPS!\n",
      "TOTAL REWARD: 609.5684003055095\n",
      "TOTAL LOSS (ACTOR, CRITIC): [1317.8175403177738, 609.3491236940026]\n"
     ]
    }
   ],
   "source": [
    "host, port = \"127.0.0.1\", 25001\n",
    "\n",
    "# SOCK_STREAM means TCP socket\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "neural_net = ActorCritic()\n",
    "request = {\n",
    "    'instruction': 'init',\n",
    "    'action': 0,\n",
    "}\n",
    "\n",
    "def on_receive(message):\n",
    "    print(message)\n",
    "\n",
    "try:\n",
    "    # Connect to the server and send the data\n",
    "    sock.connect((host, port))\n",
    "\n",
    "    start = 0\n",
    "\n",
    "    while(True):\n",
    "        #print(request)\n",
    "        request = json.dumps(request)\n",
    "        sock.sendall(request.encode(\"utf-8\"))\n",
    "        response = sock.recv(1024).decode(\"utf-8\")\n",
    "        #print(f\"Took {time() - start:.4f} between, that's {1/(time() - start):.2f} Hz\")\n",
    "        start = time()\n",
    "        request = handle_response(response, neural_net)\n",
    "finally:\n",
    "    sock.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
